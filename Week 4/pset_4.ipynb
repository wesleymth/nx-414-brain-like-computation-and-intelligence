{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### NX-414: Brain-like computation and intelligence, Spring 2025\n",
    "\n",
    "Notebook prepared by Bartlomiej Borzyszkowski; edited by Merkourios Simos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc": true
   },
   "source": [
    "# Week 3 - Grid cells and place cells in path integration\n",
    "### Table of Contents<span class=\"tocSkip\"></span>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li>\n",
    "    <li><span><a href=\"#2.-Place-cells\" data-toc-modified-id=\"Place-cells-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Place cells</a></span></li>\n",
    "    <li><span><a href=\"#3.-Trajectory-generation\" data-toc-modified-id=\"Trajectory-generation-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Trajectory generation</a></span></li>     \n",
    "    <li><span><a href=\"#4.-Recurrent-Neural-Network\" data-toc-modified-id=\"Recurrent-Neural-Network-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Recurrent Neural Network</a></span></li>\n",
    "    <li><span><a href=\"#5.-Train-the-model\" data-toc-modified-id=\"Train-the-model-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Train the model</a></span></li>  \n",
    "    <li><span><a href=\"#6.-Evaluate-performance\" data-toc-modified-id=\"Evaluate-performance-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Evaluate performance</a></span></li>  \n",
    "    <li><span><a href=\"#7.-Compute-ratemaps\" data-toc-modified-id=\"Compute-ratemaps-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Compute ratemaps</a></span></li>  \n",
    "    <li><span><a href=\"#8.-Compute-grid-scores\" data-toc-modified-id=\"Compute-grid-scores-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Compute grid scores</a></span></li> \n",
    "    <li><span><a href=\"#9.-Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Conclusion</a></span></li>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "Grid cells are a type of neuron within the medial entorhinal cortex (MEC) that allow to construct an internal map of space in the brain and fire in regular hexagonal patterns. Their remarkable discovery was awarded a Nobel Prize in Physiology or Medicine in 2014. There are currently two seemingly unrelated frameworks for understanding the patterns of grid cells. Mechanistic models account for hexagonal firing fields as the result of pattern-forming dynamics in a recurrent neural network. Normative models specify a neural architecture, a learning rule, and a navigational task, and observe that grid-like firing fields emerge due to the constraints of solving this task.\n",
    "\n",
    "In this exercise, we will study a theory that combines the two frameworks by representing the learning dynamics of a neural network trained on path integration task. In particular, we will simulate an animal moving in a square box and analyze how it performs path integration.\n",
    "\n",
    "The exercise is based on the work by [B. Sorscher et al. \"A unified theory for the origin of grid cells through the lens of pattern formation\", NeurIPS 2019.](https://papers.nips.cc/paper/2019/file/6e7d5d259be7bf56ed79029c4e621f44-Paper.pdf) We thank the authors for sharing their code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external packages\n",
    "%pip install torch imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules required for this problem set\n",
    "from src.place_cells import PlaceCells\n",
    "from src.scores import GridScorer\n",
    "from src.trainer import Trainer\n",
    "from src.trajectory_generator import TrajectoryGenerator\n",
    "from src.utils import generate_run_ID, load_trained_weights\n",
    "from src.visualize import compute_ratemaps, plot_ratemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the training configuration for the neural network. Please leave this part of code unchanged. After completing the exercise, you can play around with these parameters and check how they affect your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    \"\"\" Training options and hyperparameters \"\"\"\n",
    "    def __init__(self):\n",
    "        self.save_dir = 'models/'  # directory in which the pre-trained weights are stored\n",
    "        self.n_steps = 1000        # number of training steps\n",
    "        self.batch_size = 200      # number of trajectories per batch\n",
    "        self.sequence_length = 20  # number of steps in trajectory\n",
    "        self.learning_rate = 1e-4  # gradient descent learning rate\n",
    "        self.Np = 512              # number of place cells\n",
    "        self.Ng = 4096             # number of grid cells\n",
    "        self.place_cell_rf = 0.12  # width of place cell center tuning curve (m)\n",
    "        self.surround_scale = 2    # if DoG, ratio of sigma2^2 to sigma1^2\n",
    "        self.weight_decay = 1e-4   # strength of weight decay on recurrent weights\n",
    "        self.DoG = True            # use difference of gaussians tuning curves\n",
    "        self.box_width = 2.2       # width of training environment\n",
    "        self.box_height = 2.2      # height of training environment\n",
    "        # If your machine supports either CUDA or MPS (Apple Metal Performance Shaders), \n",
    "        # move the computation to the respective backend.\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            # Taken from https://pytorch.org/docs/stable/notes/mps.html\n",
    "            # Check that MPS is available\n",
    "            if not torch.backends.mps.is_available():\n",
    "                if not torch.backends.mps.is_built():\n",
    "                    print(\"MPS not available because the current PyTorch install was not \"\n",
    "                        \"built with MPS enabled.\")\n",
    "                    self.device = 'cpu'\n",
    "                else:\n",
    "                    print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                        \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "                    self.device = 'cpu'\n",
    "\n",
    "            else:\n",
    "                self.device = 'mps'\n",
    "        \n",
    "options = Options()\n",
    "options.run_ID = generate_run_ID(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Place cells\n",
    "Place cells are a type of neuron that are found in the hippocampus of the brain and play a role in spatial navigation and memory. They are so named because each cell is selectively activated when an animal is in a specific location within its environment, and together they form a map of the animal's surroundings. When a place cell is activated, it generates a burst of electrical activity, which can be recorded and used to determine the location of the animal. This discovery was one of the first pieces of evidence to support the idea that the brain contains a cognitive map, or a mental representation, of the environment. Place cells are thought to be involved in various functions including memory formation, recall, and spatial navigation, and are thus of interest in understanding both normal brain function and the mechanisms underlying some forms of neurological and psychiatric disorders.\n",
    "\n",
    "\n",
    "\n",
    "In this exercise, we simulate an animal moving in a square box (2.2m x 2.2m), and record the activity of simulated place cells tiled randomly and uniformly throughout the environment. We will collect the place cell activations at <em>$n_x$</em> locations as the animal explores its environment (Fig. 1). The place cell activations will be stored in a matrix $P \\in \\mathbb{R} ^ {n_x \\times n_P}$.\n",
    "\n",
    "<figure>\n",
    "  <p style=\"text-align:center;\">\n",
    "  <img src=\"img/img1.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     width=\"400\" height=\"300\"/>\n",
    "  <figcaption>Fig. 1: (Left) Simulated animal trajectory; (Right) The place cell centers (dots) of the desired place code in the environment. Blue to red indicate low to high firing rates when the animal is at the location on the left.</figcaption></p>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Implement a class of place cells that you will use in the following part of the exerice.\n",
    "\n",
    "Comple two methods for the PlaceCellsImplementation class:\n",
    "1) <em>get_nearest_cell_pos</em> - should decode position of the animal using centers of its $k$ maximally active place cells;\n",
    "2) <em>grid_pc</em> - should interpolate place cell outputs onto an environment size grid; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceCellsImplementation(PlaceCells):\n",
    "    \"\"\" Implementation of methods for Place Cells \"\"\"\n",
    "    \n",
    "    def get_nearest_cell_pos(self, activation, k=3):\n",
    "        \"\"\"\n",
    "        Decode position using centers of k maximally active place cells.\n",
    "\n",
    "        Args: \n",
    "            activation: Place cell activations of shape [batch_size, sequence_length, Np].\n",
    "            k: Number of maximally active place cells with which to decode position.\n",
    "\n",
    "        Returns:\n",
    "            pred_pos: Predicted 2d position with shape [batch_size, sequence_length, 2].\n",
    "        \"\"\"\n",
    "        # TODO: get the k largest elements of the given input tensor with activations (~1 line):\n",
    "        _, idxs =\n",
    "        \n",
    "        pred_pos = self.us[idxs].mean(-2)\n",
    "        return pred_pos\n",
    "\n",
    "    def grid_pc(self, pc_outputs, res=32):\n",
    "        \"\"\" Interpolate place cell outputs onto a grid \"\"\"\n",
    "        coordsx = np.linspace(-self.box_width/2, self.box_width/2, res)\n",
    "        coordsy = np.linspace(-self.box_height/2, self.box_height/2, res)\n",
    "        \n",
    "        # TODO: Based on coordinates x and y, create a meshgrid. \n",
    "        # Specifically, create coordinate arrays from coordinate vectors (~1 line):\n",
    "        grid_x, grid_y =\n",
    "        \n",
    "        grid = np.stack([grid_x.ravel(), grid_y.ravel()]).T\n",
    "\n",
    "        # Convert to numpy\n",
    "        pc_outputs = pc_outputs.reshape(-1, self.Np)\n",
    "\n",
    "        T = pc_outputs.shape[0]\n",
    "        pc = np.zeros([T, res, res])\n",
    "        for i in range(len(pc_outputs)):\n",
    "            gridval = scipy.interpolate.griddata(self.us.cpu(), pc_outputs[i], grid)\n",
    "            pc[i] = gridval.reshape([res, res])\n",
    "\n",
    "        return pc\n",
    "\n",
    "place_cells = PlaceCellsImplementation(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Trajectory generation\n",
    "\n",
    "Path integration refers to the process of integrating instantaneous velocity signals to obtain an estimate of current position, and is thought to be a computational function mediated by grid cells in MEC. \n",
    "\n",
    "As explained before, we want to simulate an animal moving in a box and analyze the activity of its simulated place cells. We use a random walk to generate example trajectories of the animal. As the animal moves, different subsets of its place cells should become active (Fig. 1 - right).\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Run the code below to generate the trajectories and visualize them together with place cells defined before:                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the trajectories as a random walk in a rectangular box\n",
    "trajectory_generator = TrajectoryGenerator(options, place_cells)\n",
    "\n",
    "# Plot a few sample trajectories\n",
    "inputs, pos, pc_outputs = trajectory_generator.get_test_batch()\n",
    "us = place_cells.us.cpu()\n",
    "pos = pos.cpu()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(us[:,0], us[:,1], c='lightgrey', label='Place cell centers')\n",
    "for i in range(5):\n",
    "    plt.plot(pos[:,i,0],pos[:,i,1], label='Simulated trajectory', c='C1')\n",
    "    if i==0:\n",
    "        plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few place cell outputs\n",
    "pc_outputs = pc_outputs.reshape(-1, options.Np).detach().cpu()\n",
    "pc = place_cells.grid_pc(pc_outputs[::100], res=100)\n",
    "\n",
    "plt.figure(figsize=(16,2))\n",
    "for i in range(8):\n",
    "    plt.subplot(1,8,i+1)\n",
    "    plt.imshow(pc[i], cmap='jet')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Place cell outputs', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recurrent Neural Network\n",
    "\n",
    "We train an RNN to encode position by path integrating velocity inputs. At each time step, the network receives the animal’s 2-dimensional velocity as input. The velocity signal is integrated by the network’s recurrently connected units, and the network’s\n",
    "current position representation is linearly read out into a layer of estimated place cells.\n",
    "\n",
    "The simple RNN architecture has two key advantages: \n",
    "- the simulated grid cells are recurrently connected, like grid cells in MEC\n",
    "- this architecture corresponds exactly to traditional path integrator models of grid cells known in literature\n",
    "\n",
    "\n",
    "\n",
    "We aim to investigate if this recurrent neural network develops grid-like firing fields. We show a schematic of the position encoding objective (Fig. 2). The network should learn hidden representations <em>G</em> that convert velocity inputs to a place code <em>P</em> through a set of trained read-out weights <em>W</em>.\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <p style=\"text-align:center;\">\n",
    "  <img src=\"img/img2.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     width=\"500\" height=\"600\"/>\n",
    "  <figcaption>Fig. 2: The model takes a velocity input which is fed to a recurrently connected\n",
    "hidden unit. It generates a desired place cell output representation.</figcaption></p>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Implement a vanilla RNN to encode position by path integrating velocity inputs. \n",
    "The model should consist of a single hidden layer of recurrently connected neurons with ReLU nonlinearity. Encoder and decorder should be represented as single fully-connected layers of appropriete shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\" Implementation of the Recurrent Neural Network \"\"\"\n",
    "    def __init__(self, options, place_cells):\n",
    "        super(RNN, self).__init__()\n",
    "        self.place_cells = place_cells\n",
    "            \n",
    "        # TODO: define parameters of the RNN using the pre-defined options (~4 lines):\n",
    "        self.Ng = options.Ng\n",
    "        self.Np = options.Np\n",
    "        self.sequence_length = options.sequence_length\n",
    "        self.weight_decay = options.weight_decay\n",
    "       \n",
    "        # TODO: implement an encoder as a linear layer that takes input of place cell shape and output of grid cell shape.\n",
    "        # Remark: Remember to disable an additive bias (~1 line):\n",
    "        self.encoder = \n",
    "        \n",
    "        # TODO: implement a recurrent later with input_size=2, hidden_size of grid cell shape, and ReLU activation. \n",
    "        # Remark: Remember to disable an additive bias (~1 line):\n",
    "        self.RNN = \n",
    "        \n",
    "        # TODO: implement a decoder as a linear layer that takes input of grid cell shape and output of place cell shape.\n",
    "        # Remark: Remember to disable an additive bias (~1 line):\n",
    "        self.decoder = \n",
    "        \n",
    "        # We apply softmax output to convert a vector of numbers into a vector of probabilities\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def g(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute grid cell activations.\n",
    "        Args:\n",
    "            inputs: Batch of 2d velocity inputs with shape [batch_size, sequence_length, 2].\n",
    "\n",
    "        Returns: \n",
    "            g: Batch of grid cell activations with shape [batch_size, sequence_length, Ng].\n",
    "        \"\"\"\n",
    "        v, p0 = inputs\n",
    "        init_state = self.encoder(p0)[None]\n",
    "        g, _ = self.RNN(v, init_state)\n",
    "        return g\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict place cell code.\n",
    "        Args:\n",
    "            inputs: Batch of 2d velocity inputs with shape [batch_size, sequence_length, 2].\n",
    "\n",
    "        Returns: \n",
    "            place_preds: Predicted place cell activations with shape [batch_size, sequence_length, Np].\n",
    "        \"\"\"\n",
    "        # TODO: Compute grid cell activations for a given input batch (~1 line):\n",
    "        grid_activations = \n",
    "        \n",
    "        # TODO: Use decoder to predict the place cell activations based on grid activations (~1 line):\n",
    "        place_preds = \n",
    "        \n",
    "        return place_preds\n",
    "\n",
    "    def compute_loss(self, inputs, pc_outputs, pos):\n",
    "        \"\"\"\n",
    "        Compute avg. loss and decoding error.\n",
    "        Args:\n",
    "            inputs: Batch of 2d velocity inputs with shape [batch_size, sequence_length, 2].\n",
    "            pc_outputs: Ground truth place cell activations with shape [batch_size, sequence_length, Np].\n",
    "            pos: Ground truth 2d position with shape [batch_size, sequence_length, 2].\n",
    "\n",
    "        Returns:\n",
    "            loss: Avg. loss for this training batch.\n",
    "            err: Avg. decoded position error in cm.\n",
    "        \"\"\"\n",
    "        # Compute the prediction\n",
    "        y = pc_outputs\n",
    "        preds = self.predict(inputs)\n",
    "        yhat = self.softmax(self.predict(inputs))\n",
    "        \n",
    "        # TODO: compute the cross-entropy loss for the prediction and ground truth place cell activations (~1 line):\n",
    "        loss = \n",
    "\n",
    "        # Weight regularization \n",
    "        loss += self.weight_decay * (self.RNN.weight_hh_l0**2).sum()\n",
    "\n",
    "        # Compute decoding error\n",
    "        pred_pos = self.place_cells.get_nearest_cell_pos(preds)\n",
    "        \n",
    "        # TODO: compute the RMSE for the predicted and ground truth positions (~1 line):\n",
    "        err = \n",
    "\n",
    "        return loss, err\n",
    "\n",
    "    \n",
    "model = RNN(options, place_cells).to(options.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model\n",
    "We train the “vanilla” RNN with the discrete-time dynamics:\n",
    "\n",
    "$r_i^{t+1} = \\sigma [\\sum_{j=1}^{n_g} J_{ij}r_j^t + M_{ix}v_x^t + M_{iy}v_y^t]$\n",
    "\n",
    "where:\n",
    "- $r^t$ is the population activity at time $t$;\n",
    "- $J$ is the $(ng, ng)$ recurrent connectivity matrix; \n",
    "- $\\overrightarrow{v}^t$ is a $2$-dimensional velocity input at time $t$; \n",
    "- $M$ is the $(ng, 2)$ matrix of velocity input weights; \n",
    "- $\\sigma$ is a pointwise ReLU nonlinearity. \n",
    "\n",
    "Predicted place cell outputs $\\hat{p}^t$ are read out linearly by a $(np, ng)$ matrix of weights $W$:\n",
    "\n",
    "$\\hat{p}_i^t = \\sum_{j=1}^{n_g} W_{ij}r_j^t$\n",
    "\n",
    "Rather than tuning the weights $J, M, W$ by hand, we will train them by gradient descent on\n",
    "the objective of reconstructing the true place cell outputs $p^t$ as accurately as possible. We will use a cross-entropy loss  function for this task.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Implement the training procedure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerImplementation(Trainer):\n",
    "    \"\"\" Implementation of methods for the Trainer \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_data(iterator):\n",
    "        # TODO: Find the simple command the get the next item from an iterator (~1 line):\n",
    "        return next(iterator)\n",
    "    \n",
    "    def train(self, n_epochs: int = 100, n_steps=1000, resample_trajectory=False):\n",
    "        \"\"\"\n",
    "        Train model on simulated trajectories.\n",
    "\n",
    "        Args:\n",
    "            n_epochs: Number of training epochs\n",
    "            n_steps: Number of training steps\n",
    "        \"\"\"\n",
    "        # Construct a trajectory generator\n",
    "        traj_iterator = self.trajectory_generator.get_generator()\n",
    "\n",
    "        inputs, pc_outputs, pos = self.sample_data(traj_iterator)\n",
    "\n",
    "        # Run the training loop\n",
    "        for epoch_idx in range(n_epochs):\n",
    "            for step_idx in range(n_steps):\n",
    "                \n",
    "                # TODO: implement a single training step (~1 line):\n",
    "                loss, err = self.train_step(inputs, pc_outputs, pos)\n",
    "                \n",
    "                self.loss.append(loss)\n",
    "                self.err.append(err)\n",
    "\n",
    "                if resample_trajectory:\n",
    "                    inputs, pc_outputs, pos = self.sample_data(traj_iterator)\n",
    "\n",
    "            print('Epoch: {}/{}. Step {}/{}. Loss: {}. Err: {}cm'.format(\n",
    "                epoch_idx + 1, n_epochs, step_idx + 1, n_steps,\n",
    "                np.round(loss, 2), np.round(100 * err, 2)))\n",
    "    \n",
    "    def train_step(self, inputs, pc_outputs, pos):\n",
    "        \"\"\"\n",
    "        Train on one batch of trajectories.\n",
    "\n",
    "        Args:\n",
    "            inputs: Batch of 2d velocity inputs with shape [batch_size, sequence_length, 2].\n",
    "            pc_outputs: Ground truth place cell activations with shape \n",
    "                [batch_size, sequence_length, Np].\n",
    "            pos: Ground truth 2d position with shape [batch_size, sequence_length, 2].\n",
    "\n",
    "        Returns:\n",
    "            loss: Avg. loss for this training batch.\n",
    "            err: Avg. decoded position error in cm.\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # TODO: compute loss of the model; note that you implemented it in the RNN class (~1 line)\n",
    "        loss, err = self.model.compute_loss(inputs, pc_outputs, pos)\n",
    "        \n",
    "        # TODO: perform backpropagation on the loss function and run a step of the optimizer (~2 lines):\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), err.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure is time-consuming and requires access to a powerful GPU. For the purposes of this notebook, we provide pre-trained weights of the neural network that can be used to analyze the results. \n",
    "\n",
    "Before we do this, we will demonstrate the training procedure. We will use the same model architecture to overfit on a single trajectory sample. We will plot the decoding error and a loss function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the code below to initialize a new model and train it on a single trajectory (~5-10 minutes):\n",
    "\n",
    "While you are waiting for the training to finish, here's an optional exercise: in computational neuroscience, it is often the case that results depend heavily on technical implementation details, such as the choice of hyperparameters, and so on. This has two implications:\n",
    "- First, it is important not to be let down immediately if your results are not as expected. Pay attention to the theoretical robustness of your idea to build confidence in it, and then try to understand which aspects of your implementation might be causing the problem.\n",
    "- Second, it is important to learn how to reproduce results from the literature. This way, you can avoid reinventing the wheel or falling into common pitfalls. More importantly, you can gain a lot of insight by understanding how the authors of a paper have designed their experiments and what choices they have made.\n",
    "\n",
    "So, in the spirit of the above points, here's the exercise. Following the paper link at the bottom of this notebook, look for the hyperparameters used by the authors in training their RNN. This information will probably not be in the main text; it is important to always look at the supplementary material of a paper, even when the source code is available! When you find the answer, fill in the line below the training block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerImplementation(options, model, trajectory_generator, restore=False)\n",
    "trainer.train(n_epochs=100, n_steps=10, resample_trajectory=False)\n",
    "\n",
    "# Plot the loss and error\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(trainer.err, c='black')\n",
    "\n",
    "plt.title('Decoding error (m)'); plt.xlabel('train step')\n",
    "plt.subplot(122)\n",
    "plt.plot(trainer.loss, c='black');\n",
    "plt.title('Loss'); plt.xlabel('train step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optional exercise! (just a string will do)\n",
    "options.optimizer ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the code below to load the pre-trained weights of the RNN:\n",
    "\n",
    "We've now seen that our model starts to slowly learn some representation, but this is only for one trajectory and is still far from the ideal result. To save time, we will now load the pre-trained weights and analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set restore to True to load the pre-trained weights\n",
    "trainer = TrainerImplementation(options, model, trajectory_generator, restore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate performance\n",
    "\n",
    "Once the model learned how to interpret the velocity input and generate a desired place cell output representation, we will evaluate its predictions.\n",
    "\n",
    "##### Run the code below to compare the true position of the animal to the decoded position by the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, pos, pc_outputs = trajectory_generator.get_test_batch()\n",
    "pos = pos.cpu()\n",
    "pred_pos = place_cells.get_nearest_cell_pos(model.predict(inputs)).cpu()\n",
    "us = place_cells.us.cpu()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(5):\n",
    "    plt.plot(pos[:,i,0], pos[:,i,1], c='black', label='True position', linewidth=2)\n",
    "    plt.plot(pred_pos[:,i,0], pred_pos[:,i,1], '.-',\n",
    "             c='C1', label='Decoded position')\n",
    "    if i==0:\n",
    "        plt.legend()\n",
    "plt.scatter(us[:,0], us[:,1], s=20, alpha=0.5, c='lightgrey')\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(3)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim([-options.box_width/2,options.box_width/2])\n",
    "plt.ylim([-options.box_height/2,options.box_height/2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the code below to compare the true place cell outputs to estimated place cell outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, pos, pc_outputs = trajectory_generator.get_test_batch()\n",
    "preds = model.predict(inputs)\n",
    "preds = preds.reshape(-1, options.Np).detach().cpu()\n",
    "pc_outputs = model.softmax(pc_outputs).reshape(-1, options.Np).cpu()\n",
    "pc_pred = place_cells.grid_pc(preds[:100])\n",
    "pc = place_cells.grid_pc(pc_outputs[:100])\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(8):\n",
    "    plt.subplot(2,8,i+9)\n",
    "    plt.imshow(pc_pred[2*i], cmap='jet')\n",
    "    if i==0:\n",
    "        plt.ylabel('Predicted')\n",
    "    plt.axis('off')\n",
    "    \n",
    "for i in range(8):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(pc[2*i], cmap='jet', interpolation='gaussian')\n",
    "    if i==0:\n",
    "        plt.ylabel('True')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.suptitle('Place cell outputs', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compute ratemaps\n",
    "\n",
    "We will compute a spatial ratemap for each neuron by binning the agent’s position into 2cm× 2cm bins, and computing the average firing rate within each bin.\n",
    "\n",
    "##### Run the code below to compute and plot the ratemaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 50\n",
    "n_avg = 100\n",
    "Ng = options.Ng\n",
    "activations, rate_map, g, pos = compute_ratemaps(model,\n",
    "                                                 trajectory_generator,\n",
    "                                                 options,\n",
    "                                                 res=res,\n",
    "                                                 n_avg=n_avg,\n",
    "                                                 Ng=Ng)\n",
    "\n",
    "# Compute a set of lo-res maps to use for evalutaing grid score\n",
    "lo_res = 20\n",
    "_, rate_map_lores, _, _ = compute_ratemaps(model,\n",
    "                                         trajectory_generator,\n",
    "                                         options,\n",
    "                                         res=lo_res,\n",
    "                                         n_avg=n_avg,\n",
    "                                         Ng=Ng)\n",
    "n_plot = 256\n",
    "plt.figure(figsize=(16,4*n_plot//8**2))\n",
    "rm_fig = plot_ratemaps(activations, n_plot, smooth=True)\n",
    "plt.imshow(rm_fig)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Compute grid scores\n",
    "\n",
    "Based on the ratemaps, we will compute a Grid Score by rotating a circular sample of the spatial autocorrelogram of the ratemap, and computing the correlation between the rotated map and the original. \n",
    "\n",
    "The grid score is defined as the minimum difference between the correlation at the expected peak ($60^{\\circ}$), and the correlation at the expected troughs ($90^{\\circ}$).\n",
    "\n",
    "##### Run the code below to compute and plot the Grid Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [0.2] * 10\n",
    "ends = np.linspace(0.4, 1.0, num=10)\n",
    "box_width=options.box_width\n",
    "box_height=options.box_height\n",
    "coord_range=((-box_width/2, box_width/2), (-box_height/2, box_height/2))\n",
    "masks_parameters = zip(starts, ends.tolist())\n",
    "scorer = GridScorer(20, coord_range, masks_parameters)\n",
    "\n",
    "score_60, score_90, max_60_mask, max_90_mask, sac, max_60_ind = zip(\n",
    "     *[scorer.get_scores(rm.reshape(20, 20)) for rm in tqdm(rate_map_lores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.flip(np.argsort(score_60))\n",
    "Ng = options.Ng\n",
    "\n",
    "# Plot high grid scores\n",
    "n_plot = 128\n",
    "plt.figure(figsize=(16,4*n_plot//8**2))\n",
    "rm_fig = plot_ratemaps(activations[idxs], n_plot, smooth=True)\n",
    "plt.imshow(rm_fig)\n",
    "plt.suptitle('Grid scores '+str(np.round(score_60[idxs[0]], 2))\n",
    "             +' -- '+ str(np.round(score_60[idxs[n_plot]], 2)),\n",
    "            fontsize=16)\n",
    "plt.axis('off');\n",
    "\n",
    "# Plot medium grid scores\n",
    "plt.figure(figsize=(16,4*n_plot//8**2))\n",
    "rm_fig = plot_ratemaps(activations[idxs[Ng//4:]], n_plot, smooth=True)\n",
    "plt.imshow(rm_fig)\n",
    "plt.suptitle('Grid scores '+str(np.round(score_60[idxs[Ng//2]], 2))\n",
    "             +' -- ' + str(np.round(score_60[idxs[Ng//2+n_plot]], 2)),\n",
    "            fontsize=16)\n",
    "plt.axis('off');\n",
    "\n",
    "# Plot low grid scores\n",
    "plt.figure(figsize=(16,4*n_plot//8**2))\n",
    "rm_fig = plot_ratemaps(activations[np.flip(idxs)], n_plot, smooth=True)\n",
    "plt.imshow(rm_fig)\n",
    "plt.suptitle('Grid scores '+str(np.round(score_60[idxs[-n_plot]], 2))\n",
    "             +' -- ' + str(np.round(score_60[idxs[-1]], 2)),\n",
    "            fontsize=16)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a histogram of the computed Grid Scores. How would you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score_60, range=(-1,2.5), bins=15);\n",
    "plt.xlabel('Grid score')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "The presented theory and results suggest that much of the diversity and structure of solutions obtained across many different normative models of MEC can be explained by learning\n",
    "the dynamics of a simple place cell encoding objective. The problem set showed that hexagonal grids emerge in the RNN trained to path integrate. The model yielded highly regular, stable grid cell patterns by solving the position encoding objective. \n",
    "\n",
    "Overall, the presented unifying pattern formation framework spans both normative and mechanistic models. It is a powerful conceptual tool to address many questions about the origins, structure, variability and robustness of grid-like representations in the brain.\n",
    "\n",
    "For more detail on this topic, please see the source work by [B. Sorscher et al. \"A unified theory for the origin of grid cells through the lens of pattern formation\"](https://papers.nips.cc/paper/2019/file/6e7d5d259be7bf56ed79029c4e621f44-Paper.pdf). \n",
    "##### Congratulations! You have finished this week's problem set on grid cells and place cells in path integration problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NX-414",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "342px",
    "left": "22px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
